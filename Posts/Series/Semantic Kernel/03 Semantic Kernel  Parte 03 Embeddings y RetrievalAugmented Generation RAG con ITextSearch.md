# Semantic Kernel ‚Äì Parte 03: Embeddings y Retrieval-Augmented Generation (RAG)

## **Introducci√≥n**

En esta tercera parte de nuestra serie sobre **Semantic Kernel**, nos adentramos en la integraci√≥n de **Embeddings** y **Retrieval-Augmented Generation (RAG)** para mejorar la generaci√≥n de contenido y la recuperaci√≥n de informaci√≥n. Utilizando herramientas como **Ollama** para la creaci√≥n de embeddings y **Qdrant** como vector store, podemos construir sistemas m√°s inteligentes capaces de generar respuestas m√°s precisas basadas en datos externos y actualizados, en lugar de depender √∫nicamente de los datos con los que fue entrenado el modelo. A trav√©s de esta configuraci√≥n, aprovechamos la sinergia entre el poder de los modelos de lenguaje y la capacidad de b√∫squeda sem√°ntica para proporcionar soluciones m√°s efectivas a las consultas de los usuarios. En este art√≠culo, exploraremos c√≥mo implementar estas tecnolog√≠as en **Semantic Kernel**, con ejemplos pr√°cticos para configurar los servicios y gestionar los embeddings de manera eficiente.

### **¬øQu√© son los Embeddings?**

Los **embeddings** son representaciones num√©ricas de texto en un espacio vectorial de alta dimensi√≥n. Estas representaciones permiten medir la similitud sem√°ntica entre palabras, frases o documentos completos. En el contexto de **Semantic Kernel**, los embeddings se utilizan para mejorar la b√∫squeda de informaci√≥n y la generaci√≥n de respuestas basadas en conocimiento almacenado.

### **Caracter√≠sticas clave de los Embeddings:**

- Capturan el significado sem√°ntico del texto.
- Permiten encontrar contenido relacionado aunque no se utilicen exactamente las mismas palabras.
- Se almacenan en bases de datos especializadas llamadas **vector stores** (como **Qdrant**).
- Se generan con modelos de lenguaje avanzados, como los proporcionados por **Ollama** u **Open AI**.

### **¬øQu√© es RAG (Retrieval-Augmented Generation)?**

**Retrieval-Augmented Generation (RAG)** es una t√©cnica que combina la recuperaci√≥n de informaci√≥n con la generaci√≥n de respuestas mediante un modelo de lenguaje. En lugar de confiar solo en los datos con los que fue entrenado el modelo, RAG permite buscar informaci√≥n relevante en bases de conocimiento externas y utilizarla para generar respuestas m√°s precisas y actualizadas.

#### **Funcionamiento de RAG en Semantic Kernel:**

1. **Consulta del usuario**: Un usuario hace una pregunta o solicitud.
2. **B√∫squeda sem√°ntica**:
    - Se generan embeddings de la consulta.
    - Se comparan con los embeddings almacenados en el vector store (Qdrant) para recuperar informaci√≥n relevante.
3. **Generaci√≥n de respuesta**:
    - La informaci√≥n recuperada se pasa como contexto al modelo de lenguaje.
    - Se genera una respuesta m√°s precisa y fundamentada en los datos encontrados.

### **Beneficios de RAG en Semantic Kernel:**

- **Mejor comprensi√≥n de consultas**: Permite obtener respuestas relevantes incluso si el usuario no usa palabras exactas.
- **Conocimiento actualizado**: Se pueden agregar nuevos datos al sistema sin necesidad de reentrenar el modelo de IA.
- **Optimizaci√≥n del procesamiento**: Reduce el costo computacional al recuperar solo la informaci√≥n relevante en lugar de analizar todo el corpus de datos.

## **Configurando Semantic Kernel con Embeddings y un Vector Store**

En esta secci√≥n, configuraremos **Semantic Kernel** para trabajar con **Embeddings** y un **Vector Store**, utilizando **Aspire** para orquestar los servicios.

Los servicios principales que vamos a configurar son:

- **llama3.2**: Modelo generador de texto.
- **nomic-embed-text**: Modelo generador de embeddings.  
- **Qdrant**: Almacenamiento y recuperaci√≥n de embeddings.  
- **Aspire**: Orquestaci√≥n y despliegue de los servicios.

### **Configuraci√≥n de Aspire**

Aspire nos permite definir y gestionar los servicios necesarios en un solo archivo de configuraci√≥n. En este caso, configuramos **Qdrant** como nuestro vector store y **llama3.2** y **nomic-embed-text** como proveedor de embeddings.

> Nota üí°: Esto ya lo hemos visto en los otros posts, siempre usa como referencia el c√≥digo fuente ([DevToPosts/SemanticKernelSeries/SemanticKernelLearning03 at main ¬∑ isaacOjeda/DevToPosts](https://github.com/isaacOjeda/DevToPosts/tree/main/SemanticKernelSeries/SemanticKernelLearning03)) para m√°s informaci√≥n.

```csharp
var builder = DistributedApplication.CreateBuilder(args);

// Configuraci√≥n de Qdrant
var qdrant = builder.AddQdrant("qdrant")
    .WithLifetime(ContainerLifetime.Persistent);

// Configuraci√≥n de Ollama
var ollama =
    builder.AddOllama("ollama")
        .WithDataVolume()
        .WithOpenWebUI();

// Modelos de Ollama
var llamaModel = ollama.AddModel("llama", "llama3.2");
var embedding = ollama.AddModel("embed-text", "nomic-embed-text");

// Configuraci√≥n del API Service con referencias a los modelos y Qdrant
var apiService = builder.AddProject<Projects.SemanticKernelLearning03_ApiService>("apiservice")
    .WithReference(llamaModel)
    .WithReference(embedding)
    .WithReference(qdrant);

// Construcci√≥n y ejecuci√≥n de Aspire
builder.Build().Run();
```

-  **Qdrant** se configura como un servicio persistente.
	- Se hace de esta forma para que no sea tan lento de inicializar en cada inicio de Aspire.
-  **Ollama** se configura con almacenamiento de datos y una interfaz web.  
	-  Se a√±aden los modelos **"llama3.2"** para generaci√≥n de texto y **"nomic-embed-text"** para embeddings.  
- Por √∫ltimo se agrega nuestra API en ASP.NET Core.
### **Configuraci√≥n de Semantic Kernel con Aspire**

Aspire se encargar√° de inyectar las **cadenas de conexi√≥n** en la API, seg√∫n los nombres definidos en la configuraci√≥n.

Para hacer que **Semantic Kernel** utilice correctamente los servicios orquestados por Aspire, debemos procesar estas cadenas de conexi√≥n. Como lo hemos hecho en ejemplos anteriores, parsearemos la informaci√≥n recibida y estructuraremos mejor nuestra configuraci√≥n.

Para lograr esto, **separaremos la configuraci√≥n de dependencias en un archivo espec√≠fico**, lo que nos permitir√° mantener un c√≥digo m√°s limpio y modular.

#### **Creando `DependencyConfig.cs` para gestionar las dependencias**

En este archivo, crearemos el m√©todo de extensi√≥n `AddSemanticKernel`, el cual:

- Obtiene las cadenas de conexi√≥n inyectadas por Aspire.  
- Extrae los detalles necesarios de **Ollama** (para generaci√≥n de texto y embeddings).  
- Extrae la informaci√≥n de conexi√≥n a **Qdrant** (Vector Store).  
- Registra los servicios de **Semantic Kernel** con las dependencias configuradas.  
- Agrega un servicio de **b√∫squeda sem√°ntica** (`ITextSearch`) que permite realizar b√∫squedas en una colecci√≥n de vectores, el cual usaremos m√°s adelante.

Aqu√≠ est√° el c√≥digo en `DependencyConfig.cs`:

```csharp
public static class DependencyConfig
{
    public static void AddSemanticKernel(this WebApplicationBuilder builder)
    {
        var (endpoint, completionModel) =
            GetModelDetailsFromConnectionString(builder.Configuration.GetConnectionString("llama")!);
        var (_, embeddingModel) =
            GetModelDetailsFromConnectionString(builder.Configuration.GetConnectionString("embed-text")!);
        var (host, port, apiKey) =
            GetQdrantDetailsFromConnectionString(builder.Configuration.GetConnectionString("qdrant")!);

        builder.Services.AddKernel()
            .AddOllamaChatCompletion(completionModel, endpoint)
            .AddOllamaTextEmbeddingGeneration(embeddingModel, endpoint)
            .AddQdrantVectorStore(host: host, port: port, apiKey: apiKey);

        builder.Services.AddKeyedTransient<ITextSearch>(BlogPost.VectorName, (sp, _) =>
        {
            var vectorStore = sp.GetRequiredService<IVectorStore>();
            var embeddingGenerationService = sp.GetRequiredService<ITextEmbeddingGenerationService>();

            var blogposts = vectorStore.GetCollection<Guid, BlogPost>(BlogPost.VectorName);

            blogposts.CreateCollectionIfNotExistsAsync().ConfigureAwait(true);

            var textSearch = new VectorStoreTextSearch<BlogPost>(blogposts, embeddingGenerationService);

            return textSearch;
        });
    }
}
```

**Explicaci√≥n del c√≥digo**

1. **Obtenemos los detalles de conexi√≥n**    
    - `GetModelDetailsFromConnectionString` extrae el modelo y endpoint desde las cadenas de conexi√≥n de **Ollama** (chat y embeddings).
    - `GetQdrantDetailsFromConnectionString` extrae el **host, puerto y API key** de Qdrant.
2. **Registramos los servicios de Semantic Kernel**
    - `AddOllamaChatCompletion` para generaci√≥n de texto con **Ollama**.
    - `AddOllamaTextEmbeddingGeneration` para generaci√≥n de **embeddings**.
    - `AddQdrantVectorStore` para almacenar y recuperar datos desde **Qdrant**.
3. **Configuramos un servicio de b√∫squeda sem√°ntica (`ITextSearch`)**
    - Se obtiene la colecci√≥n de **vectores** asociada a `BlogPost`.
    - Si la colecci√≥n no existe, se **crea autom√°ticamente** en Qdrant.
    - Se inicializa un objeto `VectorStoreTextSearch<BlogPost>`, permitiendo realizar **b√∫squedas sem√°nticas** en la colecci√≥n.

## **Almacenando Embeddings en Qdrant**

Para almacenar datos en Qdrant, primero debemos definir nuestro modelo de datos que ser√° indexado en la base de datos vectorial.  
En este ejemplo, queremos guardar una serie de posts y realizar b√∫squedas sem√°nticas sobre su contenido.

### **Definici√≥n del modelo `BlogPost`**

El modelo `BlogPost` representar√° las entradas del blog en nuestra base de datos vectorial.

```csharp
public class BlogPost
{
    public const string VectorName = "blogposts";

    [VectorStoreRecordKey]
    [TextSearchResultLink]
    public Guid BlogPostId { get; set; }

    [VectorStoreRecordData(IsFilterable = true)]
    [TextSearchResultName]
    public string Title { get; set; }

    [VectorStoreRecordData(IsFullTextSearchable = true)]
    [TextSearchResultValue]
    public string Description { get; set; }

    [VectorStoreRecordVector(768, DistanceFunction.DotProductSimilarity, IndexKind.Hnsw)]
    public ReadOnlyMemory<float>? DescriptionEmbedding { get; set; }

    [VectorStoreRecordData(IsFilterable = true)]
    public string[] Tags { get; set; }
}
```

**Atributos de `BlogPost`**:

- **`BlogPostId`**: Identificador √∫nico del post. Se marca como clave primaria con `[VectorStoreRecordKey]` y como enlace de resultados de b√∫squeda con `[TextSearchResultLink]`.
- **`Title`**: Almacena el t√≠tulo del post. Usado como filtro con `[VectorStoreRecordData(IsFilterable = true)]` y como nombre principal en los resultados de b√∫squeda con `[TextSearchResultName]`.
- **`Description`**: Contiene el contenido del post. Es indexado para b√∫squedas de texto completo con `[VectorStoreRecordData(IsFullTextSearchable = true)]` y su valor es mostrado en los resultados con `[TextSearchResultValue]`.
- **`DescriptionEmbedding`**: Representaci√≥n vectorial de `Description`. Almacenado como un vector de 768 dimensiones usando `[VectorStoreRecordVector]`, con `DotProductSimilarity` como funci√≥n de distancia y `Hnsw` para indexaci√≥n eficiente.
- **`Tags`**: Etiquetas relacionadas con el post. Pueden ser usadas como filtros en consultas con `[VectorStoreRecordData(IsFilterable = true)]`.
### **Creando un endpoint para almacenar `BlogPost` en Qdrant**

Para guardar nuevos `BlogPost`, crearemos un endpoint que recibir√° los datos del post, generar√° su embedding y lo almacenar√° en la base de datos vectorial.

Nuestro servicio ya tiene configurados `ITextEmbeddingGenerationService` (para generar embeddings) e `IVectorStore` (para interactuar con Qdrant).

```csharp
public static class CreateBlogPost
{
    public record Request(string Title, string Description, string[] Tags);

    public record Response(Guid BlogPostId);

    public class Handler
    {
        private readonly ITextEmbeddingGenerationService _embeddingService;
        private readonly IVectorStore _vectorStore;

        public Handler(ITextEmbeddingGenerationService embeddingService, IVectorStore vectorStore)
        {
            _embeddingService = embeddingService;
            _vectorStore = vectorStore;
        }

        public async Task<Response> Handle(Request request, CancellationToken ct)
        {
            var blogposts = _vectorStore.GetCollection<Guid, BlogPost>(BlogPost.VectorName);

            await blogposts.CreateCollectionIfNotExistsAsync(ct);

            var embeddingContents =
                await _embeddingService.GenerateEmbeddingAsync(request.Description, cancellationToken: ct);

            var newBlogPost = new BlogPost
            {
                BlogPostId = Guid.NewGuid(),
                Title = request.Title,
                Description = request.Description,
                DescriptionEmbedding = embeddingContents,
                Tags = request.Tags
            };

            await blogposts.UpsertAsync(newBlogPost, cancellationToken: ct);

            return new Response(newBlogPost.BlogPostId);
        }
    }
}
```


**Explicaci√≥n del c√≥digo**

1. **Manejador (`Handler`)**
    - Recibe los servicios `ITextEmbeddingGenerationService` y `IVectorStore` a trav√©s del constructor.
2. **L√≥gica del m√©todo `Handle`**
    - Obtiene la colecci√≥n `blogposts` de Qdrant.
    - Si la colecci√≥n no existe, la crea con `CreateCollectionIfNotExistsAsync()`.
    - Genera el embedding del `Description` usando `_embeddingService.GenerateEmbeddingAsync()`.
    - Crea un `BlogPost` con los datos proporcionados y el embedding generado.
    - Guarda el post en Qdrant con `UpsertAsync()`, que inserta o actualiza el registro.

> **Nota üí°**: Para ver c√≥mo este handler se integra con Minimal APIs, revisa el c√≥digo fuente del proyecto. Para mantener este art√≠culo conciso, omitiremos detalles espec√≠ficos.

#### **Probando el endpoint con `api.http`**

Para poblar la base de datos vectorial, podemos hacer una solicitud HTTP de prueba.

Ejemplo en `SemanticKernelLearning03.ApiService.http`:

```
@SemanticKernelLearning03.ApiService_HostAddress = https://localhost:7391

POST {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts
Content-Type: application/json

{
  "Title": "Introducci√≥n a ASP.NET Core",
  "Description": "Una gu√≠a completa sobre c√≥mo comenzar con ASP.NET Core y sus caracter√≠sticas principales.",
  "Tags": ["ASP.NET Core", "C#", "Web Development"]
}
```

### **Verificando los datos en Qdrant**

Si ejecutamos nuestra soluci√≥n con Aspire y realizamos la solicitud anterior, podemos inspeccionar los datos en el dashboard de Qdrant y verificar que la colecci√≥n y los registros existen.

#### **Consideraciones sobre el tama√±o de los vectores**

- Este ejemplo usa **nomic-embed-text**, que genera vectores de **768 dimensiones**.
- Qdrant debe estar configurado para aceptar este tama√±o de vector.
- Si usas modelos de OpenAI u otros proveedores, verifica el tama√±o del vector antes de insertarlo en Qdrant.
## **Recuperando Informaci√≥n desde Qdrant**

Para demostrar c√≥mo realizar b√∫squedas sem√°nticas en los vectores almacenados en Qdrant, podemos hacerlo de distintas maneras: usando el vector directamente o utilizando una abstracci√≥n que **Semantic Kernel** nos proporciona, llamada `ITextSearch`.

`ITextSearch` tiene distintas implementaciones, como `BingTextSearch` (que realiza b√∫squedas en Bing), pero en nuestro caso usaremos `VectorStoreTextSearch`, que est√° dise√±ado espec√≠ficamente para realizar b√∫squedas en bases de datos vectoriales.

Esta clase necesita conocer:

- **El modelo de embeddings** que estamos usando.
- **El nombre de la colecci√≥n en Qdrant** donde haremos la b√∫squeda.

El proceso realmente es sencillo y podr√≠amos hacerlo manualmente construyendo consultas directamente contra la base de datos vectorial. Sin embargo, en este caso, aprovecharemos `ITextSearch` para simplificar el proceso.

> Nota üí°: Para m√°s informaci√≥n sobre b√∫squedas vectoriales en Semantic Kernel, consulta la documentaci√≥n oficial de Microsoft:  
> [Vector search using Semantic Kernel Vector Store connectors (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/vector-search?pivots=programming-language-csharp)

### **Implementaci√≥n del Endpoint `SearchBlogPost`**

Para realizar la b√∫squeda sem√°ntica, crearemos un nuevo endpoint llamado `SearchBlogPost`, que permitir√° encontrar los **posts m√°s relevantes** basados en una consulta en lenguaje natural.

```csharp
public static class SearchBlogPost
{
    public record Request(string Query);

    public record Response(Guid BlogPostId, string Title, string Description);

    public class Handler([FromKeyedServices(BlogPost.VectorName)] ITextSearch textSearch)
    {
        public async Task<List<Response>> Handle(Request request, CancellationToken ct)
        {
            KernelSearchResults<TextSearchResult> textResults =
                await textSearch.GetTextSearchResultsAsync(request.Query, new()
                {
                    Top = 2,
                    Skip = 0,
                }, ct);

            List<Response> responses = new();
            await foreach (TextSearchResult result in textResults.Results.WithCancellation(ct))
            {
                responses.Add(new(
                    Guid.Parse(result.Link),
                    result.Name,
                    result.Value));
            }

            return responses;
        }
    }
}
```

**Explicaci√≥n del C√≥digo**

1. **Uso de `ITextSearch`**
    - Lo recibimos en el constructor con `[FromKeyedServices(BlogPost.VectorName)]`. Esto asegura que estamos inyectando la instancia correcta configurada para nuestra colecci√≥n de **blog posts** en Qdrant.
	    - Esto lo hemos configurado en `DependencyConfig` anteriormente.
2. **Realizamos la b√∫squeda sem√°ntica**
    - Llamamos a `GetTextSearchResultsAsync(...);`, donde:
        - `request.Query` es el t√©rmino de b√∫squeda ingresado.
        - `Top = 2` indica que queremos los **dos resultados m√°s relevantes**.
        - `Skip = 0` significa que no saltaremos ning√∫n resultado.
3. **Procesamos los resultados**
    - Iteramos sobre `textResults.Results` para extraer:
        - `result.Link`: Contiene el `BlogPostId` almacenado como string, lo convertimos a `Guid`.
        - `result.Name`: Corresponde al t√≠tulo del blog post.
        - `result.Value`: Contiene la descripci√≥n del blog post.
4. **Devolvemos la lista de resultados**
    - Cada `Response` representa un **blog post relevante** basado en la b√∫squeda sem√°ntica.

Una vez que el endpoint `SearchBlogPost` est√° registrado, podemos comenzar a realizar b√∫squedas sem√°nticas para encontrar los posts m√°s relevantes seg√∫n el texto ingresado.

**Ejemplo de Consulta**

Podemos hacer una b√∫squeda con la siguiente solicitud:

```
### Busquedas Semanticas
@Query=arquitectura de software
GET {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts?Query={{Query}}
```

**Respuesta Esperada**

Si hay blog posts relevantes en la base de datos, la API devolver√° un listado de los m√°s coincidentes. Un ejemplo de respuesta podr√≠a ser:

```json
[
¬†¬†{
¬†¬†¬†¬†"blogPostId":¬†"949b7164-2021-4fcc-b58e-9887fdd00d0e",
¬†¬†¬†¬†"title":¬†"Clean¬†Architecture:¬†Dise√±o¬†de¬†Software¬†Modular¬†y¬†Escalable",
¬†¬†¬†¬†"description":¬†"Clean¬†Architecture¬†es¬†un¬†enfoque¬†de¬†dise√±o¬†de¬†software¬†..."
¬†¬†},
¬†¬†{
¬†¬†¬†¬†"blogPostId":¬†"2340f1ac-e810-4067-90f5-a71899c6d42a",
¬†¬†¬†¬†"title":¬†"Principios¬†SOLID¬†en¬†el¬†Desarrollo¬†de¬†Software",
¬†¬†¬†¬†"description":¬†"Los¬†principios¬†SOLID¬†son¬†un¬†conjunto¬†de¬†cinco¬†principios¬†de..."
¬†¬†}
]
```

Si revisamos el repositorio, podemos notar que hay posts de distintos temas (por ejemplo, **Machine Learning, Bases de Datos, DevOps**), pero la b√∫squeda regres√≥ los m√°s relevantes seg√∫n el texto ingresado. Esto demuestra que el motor de b√∫squeda sem√°ntico est√° funcionando correctamente.
#### **Personalizando la B√∫squeda**

Podemos ajustar la consulta cambiando:
- **La cantidad de resultados (`Top`)**: Actualmente devuelve **dos posts**, pero podr√≠amos aumentar este valor si queremos m√°s opciones.
- **El modelo de embeddings**: Si cambiamos el modelo de embeddings usado para almacenar los vectores, los resultados pueden variar en precisi√≥n y relevancia.

## **Integraci√≥n con RAG (Retrieval-Augmented Generation)**

En esta secci√≥n, vamos a implementar un nuevo endpoint que va m√°s all√° de una b√∫squeda sem√°ntica. En lugar de solo buscar contenido relevante, vamos a generar texto de manera din√°mica a partir de una consulta proporcionada por el usuario. Lo interesante aqu√≠ es que el contexto para la generaci√≥n de la respuesta ser√° el contenido de los **Posts del Blog** que coincidan con la consulta del usuario.

### **¬øC√≥mo funciona?**

Si el usuario hace una pregunta como:

```
cu√°l es el objetivo principal de clean architecture?
```

En lugar de simplemente devolver una lista de posts relacionados, el modelo de Semantic Kernel buscar√° los posts m√°s relevantes relacionados con la consulta, y usar√° esos resultados como contexto para generar una respuesta detallada. El modelo no solo devolver√° contenido est√°tico, sino que tambi√©n incorporar√° la informaci√≥n m√°s relevante y actualizada disponible en los posts, permitiendo una respuesta m√°s precisa y contextualizada.

Esta t√©cnica es √∫til porque, por lo general, los modelos de lenguaje (LLMs) se entrenan con datos est√°ticos que no contienen informaci√≥n actualizada o espec√≠fica del dominio. Al integrar la **Recuperaci√≥n de Informaci√≥n (RAG)**, aprovechamos los datos disponibles en tiempo real para mejorar la relevancia y exactitud de las respuestas generadas.

### **¬øC√≥mo lo implementamos?**

En el post anterior, vimos c√≥mo crear **Plugins** o **Funciones Sem√°nticas**, y c√≥mo Semantic Kernel se encarga de invocar autom√°ticamente esos plugins cuando es necesario. Ahora, vamos a utilizar un plugin de b√∫squeda vectorial para recuperar los posts relevantes y luego construir un **prompt personalizado** que permita al modelo generar una respuesta coherente utilizando esos posts como contexto.

#### **El Endpoint y el C√≥digo**

El endpoint que vamos a implementar es el siguiente:

```csharp
public static class QuestionsBlogPost
{
    public record Request(string Query);

    public record Response(string Content);

    public class Handler(Kernel kernel, [FromKeyedServices(BlogPost.VectorName)] ITextSearch textSearch)
    {
        public async Task<Response> Handle(Request request, CancellationToken ct)
        {
            var searchPlugin = textSearch.CreateWithGetTextSearchResults("SearchPlugin");
            kernel.Plugins.Add(searchPlugin);

            string promptTemplate = """
                                    Utiliza solamente los siguientes contenidos para contestar las preguntas realizadas. 
                                    Si no sabes la respuesta, hazle saber al usuario que no se encontr√≥ informaci√≥n relevante.

                                    Al final de tu respuesta, incluye el nombre del contenido utilizado como referencia.

                                    {{#with (SearchPlugin-GetTextSearchResults query)}}  
                                        {{#each this}}  
                                        Name: {{Name}}
                                        Value: {{Value}}
                                        -----------------
                                        {{/each}}  
                                    {{/with}}  

                                    Pregunta:

                                    {{query}}
                                    """;

            KernelArguments arguments = new() { { "query", request.Query } };

            HandlebarsPromptTemplateFactory promptTemplateFactory = new();
            var response = await kernel.InvokePromptAsync(
                promptTemplate,
                arguments,
                templateFormat: HandlebarsPromptTemplateFactory.HandlebarsTemplateFormat,
                promptTemplateFactory: promptTemplateFactory,
                cancellationToken: ct);

            return new Response(response.ToString());
        }
    }
}

```

**Explicaci√≥n del C√≥digo**

- **`Handler`**: Esta clase maneja la l√≥gica de la consulta, invoca el plugin de b√∫squeda, crea el prompt con el contexto adecuado y genera la respuesta. Recibe un objeto **`Kernel`** y una instancia de **`ITextSearch`** que se utiliza para realizar la b√∫squeda de contenido relevante en los posts del blog.    
- **`searchPlugin`**: Este es el plugin de b√∫squeda que se crea utilizando el m√©todo **`CreateWithGetTextSearchResults`**. Este plugin se encarga de realizar la b√∫squeda sem√°ntica en los posts y recuperar los resultados m√°s relevantes. El plugin se a√±ade al **`Kernel`** para que pueda ser invocado cuando sea necesario.
- **`promptTemplate`**: Es el template que se utilizar√° para generar el texto. En este caso, se utiliza **Handlebars** para permitir la inserci√≥n de los resultados de la b√∫squeda dentro del prompt. La idea es que el modelo genere una respuesta usando solamente los contenidos encontrados en la b√∫squeda. El template tambi√©n incluye instrucciones para que el modelo devuelva el nombre y el valor de los posts utilizados como referencia.

```
{{#with (SearchPlugin-GetTextSearchResults query)}}  
    {{#each this}}  
    Name: {{Name}}  
    Value: {{Value}}  
    -----------------
    {{/each}}  
{{/with}}  
```
- Al final, se agrega la pregunta del usuario para que el modelo la utilice como parte de la respuesta:
```
Pregunta:
{{query}}
```
- **`KernelArguments`**: Este objeto contiene los argumentos que se pasan al modelo, en este caso, la consulta realizada por el usuario.
- **`InvokePromptAsync`**: Este m√©todo invoca el modelo utilizando el template y los argumentos proporcionados, lo que genera una respuesta basada en los contenidos recuperados.

**Flujo Completo**

7. **El usuario hace una consulta** (por ejemplo, "¬øQu√© es Clean Architecture?").
8. El **`Kernel`** busca los posts relevantes utilizando el **plugin de b√∫squeda vectorial**.
9. El **prompt personalizado** es generado e incluye los resultados de la b√∫squeda.
10. El **modelo LLM** genera una respuesta utilizando los posts recuperados como contexto.
11. La **respuesta generada** se devuelve al usuario, incluyendo la referencia a los contenidos utilizados.

Este enfoque permite mejorar la calidad de las respuestas generadas por el modelo, ya que se aprovecha la informaci√≥n m√°s actualizada y relevante disponible en el dominio. Adem√°s, al usar el contenido de los posts, el modelo tiene un contexto m√°s espec√≠fico y adaptado a las necesidades del usuario, lo que mejora la relevancia y precisi√≥n de la respuesta.

**Probando el Endpoint**

Una vez que hemos configurado y creado nuestro endpoint, es hora de probarlo. Para ello, podemos hacer una solicitud HTTP como la siguiente, que ilustrar√° c√≥mo interactuar con el servicio de generaci√≥n de texto mediante la integraci√≥n de RAG.

Ejemplo de solicitud HTTP:

```
###
@Question=cu√°l es el objetivo principal de clean architecture?
GET {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts/qa?Query={{Question}}
```

Cuando se ejecuta esta solicitud, obtendremos una respuesta estructurada que incluye no solo la respuesta generada por el modelo, sino tambi√©n la referencia al contenido utilizado para generar esa respuesta.

Respuesta esperada:

```
El¬†objetivo¬†principal¬†de¬†Clean¬†Architecture¬†es¬†promover¬†la¬†separaci√≥n¬†de¬†preocupaciones¬†y¬†la¬†modularidad¬†en¬†el¬†dise√±o¬†de¬†software,¬†lo¬†que¬†permite¬†a¬†las¬†aplicaciones¬†ser¬†m√°s¬†mantenibles¬†y¬†escalables¬†a¬†lo¬†largo¬†del¬†tiempo.

Reference:¬†Clean¬†Architecture:¬†Dise√±o¬†de¬†Software¬†Modular¬†y¬†Escalable.
```

En este caso, la respuesta del endpoint es completamente din√°mica y se genera utilizando los resultados de b√∫squeda obtenidos de los posts del blog relacionados con la pregunta. Como puedes observar, no solo obtenemos un resultado que responde a la consulta, sino que tambi√©n proporcionamos el nombre del contenido utilizado como referencia, lo cual permite al usuario conocer la fuente de la informaci√≥n generada.

En el ejemplo, configuramos el `promptTemplate` para que el modelo de lenguaje solo utilizara los contenidos relevantes encontrados a trav√©s de la b√∫squeda de texto vectorial. De esta forma, garantizamos que la generaci√≥n del modelo sea espec√≠fica y relevante para el contexto proporcionado por los posts del blog.

Este template establece claramente la regla para que el modelo solo use los resultados de b√∫squeda como contexto, lo cual optimiza la relevancia de las respuestas generadas.

**Escalabilidad y Flexibilidad del Prompt**

El prompt puede ser adaptado para ser m√°s espec√≠fico y mejorar la calidad de las respuestas generadas, por ejemplo, agregando instrucciones m√°s detalladas sobre el formato de la respuesta o proporcionando una estructura m√°s precisa para guiar al modelo. La flexibilidad del sistema permite ajustar el nivel de detalle seg√∫n la necesidad del caso de uso.

**Ejemplo de otro Prompt:**

```
Enlista los puntos m√°s importantes al querer optimizar consultas SQL
```

**Resultado generado:**

```
¬°Claro! A continuaci√≥n, te presento los puntos m√°s importantes para optimizar consultas SQL:

1. **√çndices**: Los √≠ndices ayudan a acelerar la b√∫squeda de datos en una base de datos. Deben ser creados en las columnas que se utilizan en la cl√°usula WHERE o en la cl√°usula ORDER BY.
2. **Normalizaci√≥n y desnormalizaci√≥n de datos**: La normalizaci√≥n ayuda a reducir la cantidad de datos duplicados en una base de datos, mientras que la desnormalizaci√≥n puede mejorar el rendimiento en ciertas situaciones.
.
.
.

Referencia: Optimizaci√≥n de Consultas SQL en Aplicaciones .NET
```

Como vemos, el modelo genera una lista de los puntos m√°s importantes y proporciona la referencia adecuada para la informaci√≥n. Este tipo de integraci√≥n con Semantic Kernel y RAG hace que sea posible tener respuestas altamente relevantes y espec√≠ficas basadas en contenido actual y relacionado.
## Conclusi√≥n

A lo largo de este post, hemos aprendido a configurar y utilizar Semantic Kernel para aprovechar el poder de los embeddings y la t√©cnica de Retrieval-Augmented Generation (RAG). Desde la generaci√≥n y almacenamiento de vectores en Qdrant, hasta la realizaci√≥n de b√∫squedas sem√°nticas y la generaci√≥n de respuestas contextualizadas mediante prompts personalizados, hemos visto c√≥mo combinar distintas tecnolog√≠as para crear soluciones de IA avanzadas y especializadas.

Esta integraci√≥n no solo mejora la precisi√≥n y relevancia de las respuestas, sino que tambi√©n demuestra el potencial de unir funciones sem√°nticas, RAG y bases de datos vectoriales para abordar desaf√≠os reales en el manejo de informaci√≥n. Con estas herramientas, podr√°s desarrollar aplicaciones que se adaptan al contexto de tus datos, ofreciendo respuestas din√°micas y actualizadas.

¬°El camino hacia aplicaciones inteligentes y contextualmente precisas est√° abierto! Sigue experimentando y optimizando estas t√©cnicas para llevar tus proyectos al siguiente nivel.

## Referencias

- [Using the Semantic Kernel Vector Store text search (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/out-of-the-box-textsearch/vectorstore-textsearch?pivots=programming-language-csharp)
- [Semantic Kernel Text Search Plugins (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/text-search-plugins?pivots=programming-language-csharp)
- [Semantic Kernel Text Search with Vector Stores (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/text-search-vector-stores?pivots=programming-language-csharp)
- [Using the Handlebars prompt template language | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/prompts/handlebars-prompt-templates?pivots=programming-language-csharp)