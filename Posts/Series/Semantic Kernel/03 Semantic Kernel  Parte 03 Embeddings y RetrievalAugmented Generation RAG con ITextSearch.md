## **Introducci칩n**

En esta tercera parte de nuestra serie sobre **Semantic Kernel**, nos adentramos en la integraci칩n de **Embeddings** y **Retrieval-Augmented Generation (RAG)** para mejorar la generaci칩n de contenido y la recuperaci칩n de informaci칩n. Utilizando herramientas como **Ollama** para la creaci칩n de embeddings y **Qdrant** como vector store, podemos construir sistemas m치s inteligentes capaces de generar respuestas m치s precisas basadas en datos externos y actualizados, en lugar de depender 칰nicamente de los datos con los que fue entrenado el modelo. A trav칠s de esta configuraci칩n, aprovechamos la sinergia entre el poder de los modelos de lenguaje y la capacidad de b칰squeda sem치ntica para proporcionar soluciones m치s efectivas a las consultas de los usuarios. En este art칤culo, exploraremos c칩mo implementar estas tecnolog칤as en **Semantic Kernel**, con ejemplos pr치cticos para configurar los servicios y gestionar los embeddings de manera eficiente.

### **쯈u칠 son los Embeddings?**

Los **embeddings** son representaciones num칠ricas de texto en un espacio vectorial de alta dimensi칩n. Estas representaciones permiten medir la similitud sem치ntica entre palabras, frases o documentos completos. En el contexto de **Semantic Kernel**, los embeddings se utilizan para mejorar la b칰squeda de informaci칩n y la generaci칩n de respuestas basadas en conocimiento almacenado.

### **Caracter칤sticas clave de los Embeddings:**

- Capturan el significado sem치ntico del texto.
- Permiten encontrar contenido relacionado aunque no se utilicen exactamente las mismas palabras.
- Se almacenan en bases de datos especializadas llamadas **vector stores** (como **Qdrant**).
- Se generan con modelos de lenguaje avanzados, como los proporcionados por **Ollama** u **Open AI**.

### **쯈u칠 es RAG (Retrieval-Augmented Generation)?**

**Retrieval-Augmented Generation (RAG)** es una t칠cnica que combina la recuperaci칩n de informaci칩n con la generaci칩n de respuestas mediante un modelo de lenguaje. En lugar de confiar solo en los datos con los que fue entrenado el modelo, RAG permite buscar informaci칩n relevante en bases de conocimiento externas y utilizarla para generar respuestas m치s precisas y actualizadas.

#### **Funcionamiento de RAG en Semantic Kernel:**

1. **Consulta del usuario**: Un usuario hace una pregunta o solicitud.
2. **B칰squeda sem치ntica**:
    - Se generan embeddings de la consulta.
    - Se comparan con los embeddings almacenados en el vector store (Qdrant) para recuperar informaci칩n relevante.
3. **Generaci칩n de respuesta**:
    - La informaci칩n recuperada se pasa como contexto al modelo de lenguaje.
    - Se genera una respuesta m치s precisa y fundamentada en los datos encontrados.

### **Beneficios de RAG en Semantic Kernel:**

- **Mejor comprensi칩n de consultas**: Permite obtener respuestas relevantes incluso si el usuario no usa palabras exactas.
- **Conocimiento actualizado**: Se pueden agregar nuevos datos al sistema sin necesidad de reentrenar el modelo de IA.
- **Optimizaci칩n del procesamiento**: Reduce el costo computacional al recuperar solo la informaci칩n relevante en lugar de analizar todo el corpus de datos.

## **Configurando Semantic Kernel con Embeddings y un Vector Store**

En esta secci칩n, configuraremos **Semantic Kernel** para trabajar con **Embeddings** y un **Vector Store**, utilizando **Aspire** para orquestar los servicios.

Los servicios principales que vamos a configurar son:

- **llama3.2**: Modelo generador de texto.
- **nomic-embed-text**: Modelo generador de embeddings.  
- **Qdrant**: Almacenamiento y recuperaci칩n de embeddings.  
- **Aspire**: Orquestaci칩n y despliegue de los servicios.

### **Configuraci칩n de Aspire**

Aspire nos permite definir y gestionar los servicios necesarios en un solo archivo de configuraci칩n. En este caso, configuramos **Qdrant** como nuestro vector store y **llama3.2** y **nomic-embed-text** como proveedor de embeddings.

> Nota 游눠: Esto ya lo hemos visto en los otros posts, siempre usa como referencia el c칩digo fuente ([DevToPosts/SemanticKernelSeries/SemanticKernelLearning03 at main 췅 isaacOjeda/DevToPosts](https://github.com/isaacOjeda/DevToPosts/tree/main/SemanticKernelSeries/SemanticKernelLearning03)) para m치s informaci칩n.

```csharp
var builder = DistributedApplication.CreateBuilder(args);

// Configuraci칩n de Qdrant
var qdrant = builder.AddQdrant("qdrant")
    .WithLifetime(ContainerLifetime.Persistent);

// Configuraci칩n de Ollama
var ollama =
    builder.AddOllama("ollama")
        .WithDataVolume()
        .WithOpenWebUI();

// Modelos de Ollama
var llamaModel = ollama.AddModel("llama", "llama3.2");
var embedding = ollama.AddModel("embed-text", "nomic-embed-text");

// Configuraci칩n del API Service con referencias a los modelos y Qdrant
var apiService = builder.AddProject<Projects.SemanticKernelLearning03_ApiService>("apiservice")
    .WithReference(llamaModel)
    .WithReference(embedding)
    .WithReference(qdrant);

// Construcci칩n y ejecuci칩n de Aspire
builder.Build().Run();
```

-  **Qdrant** se configura como un servicio persistente.
	- Se hace de esta forma para que no sea tan lento de inicializar en cada inicio de Aspire.
-  **Ollama** se configura con almacenamiento de datos y una interfaz web.  
	-  Se a침aden los modelos **"llama3.2"** para generaci칩n de texto y **"nomic-embed-text"** para embeddings.  
- Por 칰ltimo se agrega nuestra API en ASP.NET Core.
### **Configuraci칩n de Semantic Kernel con Aspire**

Aspire se encargar치 de inyectar las **cadenas de conexi칩n** en la API, seg칰n los nombres definidos en la configuraci칩n.

Para hacer que **Semantic Kernel** utilice correctamente los servicios orquestados por Aspire, debemos procesar estas cadenas de conexi칩n. Como lo hemos hecho en ejemplos anteriores, parsearemos la informaci칩n recibida y estructuraremos mejor nuestra configuraci칩n.

Para lograr esto, **separaremos la configuraci칩n de dependencias en un archivo espec칤fico**, lo que nos permitir치 mantener un c칩digo m치s limpio y modular.

#### **Creando `DependencyConfig.cs` para gestionar las dependencias**

En este archivo, crearemos el m칠todo de extensi칩n `AddSemanticKernel`, el cual:

- Obtiene las cadenas de conexi칩n inyectadas por Aspire.  
- Extrae los detalles necesarios de **Ollama** (para generaci칩n de texto y embeddings).  
- Extrae la informaci칩n de conexi칩n a **Qdrant** (Vector Store).  
- Registra los servicios de **Semantic Kernel** con las dependencias configuradas.  
- Agrega un servicio de **b칰squeda sem치ntica** (`ITextSearch`) que permite realizar b칰squedas en una colecci칩n de vectores, el cual usaremos m치s adelante.

Aqu칤 est치 el c칩digo en `DependencyConfig.cs`:

```csharp
public static class DependencyConfig
{
    public static void AddSemanticKernel(this WebApplicationBuilder builder)
    {
        var (endpoint, completionModel) =
            GetModelDetailsFromConnectionString(builder.Configuration.GetConnectionString("llama")!);
        var (_, embeddingModel) =
            GetModelDetailsFromConnectionString(builder.Configuration.GetConnectionString("embed-text")!);
        var (host, port, apiKey) =
            GetQdrantDetailsFromConnectionString(builder.Configuration.GetConnectionString("qdrant")!);

        builder.Services.AddKernel()
            .AddOllamaChatCompletion(completionModel, endpoint)
            .AddOllamaTextEmbeddingGeneration(embeddingModel, endpoint)
            .AddQdrantVectorStore(host: host, port: port, apiKey: apiKey);

        builder.Services.AddKeyedTransient<ITextSearch>(BlogPost.VectorName, (sp, _) =>
        {
            var vectorStore = sp.GetRequiredService<IVectorStore>();
            var embeddingGenerationService = sp.GetRequiredService<ITextEmbeddingGenerationService>();

            var blogposts = vectorStore.GetCollection<Guid, BlogPost>(BlogPost.VectorName);

            blogposts.CreateCollectionIfNotExistsAsync().ConfigureAwait(true);

            var textSearch = new VectorStoreTextSearch<BlogPost>(blogposts, embeddingGenerationService);

            return textSearch;
        });
    }
}
```

**Explicaci칩n del c칩digo**

1. **Obtenemos los detalles de conexi칩n**    
    - `GetModelDetailsFromConnectionString` extrae el modelo y endpoint desde las cadenas de conexi칩n de **Ollama** (chat y embeddings).
    - `GetQdrantDetailsFromConnectionString` extrae el **host, puerto y API key** de Qdrant.
2. **Registramos los servicios de Semantic Kernel**
    - `AddOllamaChatCompletion` para generaci칩n de texto con **Ollama**.
    - `AddOllamaTextEmbeddingGeneration` para generaci칩n de **embeddings**.
    - `AddQdrantVectorStore` para almacenar y recuperar datos desde **Qdrant**.
3. **Configuramos un servicio de b칰squeda sem치ntica (`ITextSearch`)**
    - Se obtiene la colecci칩n de **vectores** asociada a `BlogPost`.
    - Si la colecci칩n no existe, se **crea autom치ticamente** en Qdrant.
    - Se inicializa un objeto `VectorStoreTextSearch<BlogPost>`, permitiendo realizar **b칰squedas sem치nticas** en la colecci칩n.

## **Almacenando Embeddings en Qdrant**

Para almacenar datos en Qdrant, primero debemos definir nuestro modelo de datos que ser치 indexado en la base de datos vectorial.  
En este ejemplo, queremos guardar una serie de posts y realizar b칰squedas sem치nticas sobre su contenido.


> Advertencia 丘멆잺: Las b칰squedas vectoriales que estamos viendo aqu칤 est치n en **Preview (Alpha)**, por lo que podr칤an cambiar en el futuro. Actualmente, **no se recomienda su uso en producci칩n**, a menos que est칠s dispuesto a evolucionar junto con el framework.
> Anteriormente, se utilizaban los **Memory de Semantic Kernel** para b칰squedas similares, pero estos est치n en proceso de ser considerados **"Legacy"**, por lo que su uso a largo plazo podr칤a no ser viable.

### **Definici칩n del modelo `BlogPost`**

El modelo `BlogPost` representar치 las entradas del blog en nuestra base de datos vectorial.

```csharp
public class BlogPost
{
    public const string VectorName = "blogposts";

    [VectorStoreRecordKey]
    [TextSearchResultLink]
    public Guid BlogPostId { get; set; }

    [VectorStoreRecordData(IsFilterable = true)]
    [TextSearchResultName]
    public string Title { get; set; }

    [VectorStoreRecordData(IsFullTextSearchable = true)]
    [TextSearchResultValue]
    public string Description { get; set; }

    [VectorStoreRecordVector(768, DistanceFunction.DotProductSimilarity, IndexKind.Hnsw)]
    public ReadOnlyMemory<float>? DescriptionEmbedding { get; set; }

    [VectorStoreRecordData(IsFilterable = true)]
    public string[] Tags { get; set; }
}
```

**Atributos de `BlogPost`**:

- **`BlogPostId`**: Identificador 칰nico del post. Se marca como clave primaria con `[VectorStoreRecordKey]` y como enlace de resultados de b칰squeda con `[TextSearchResultLink]`.
- **`Title`**: Almacena el t칤tulo del post. Usado como filtro con `[VectorStoreRecordData(IsFilterable = true)]` y como nombre principal en los resultados de b칰squeda con `[TextSearchResultName]`.
- **`Description`**: Contiene el contenido del post. Es indexado para b칰squedas de texto completo con `[VectorStoreRecordData(IsFullTextSearchable = true)]` y su valor es mostrado en los resultados con `[TextSearchResultValue]`.
- **`DescriptionEmbedding`**: Representaci칩n vectorial de `Description`. Almacenado como un vector de 768 dimensiones usando `[VectorStoreRecordVector]`, con `DotProductSimilarity` como funci칩n de distancia y `Hnsw` para indexaci칩n eficiente.
- **`Tags`**: Etiquetas relacionadas con el post. Pueden ser usadas como filtros en consultas con `[VectorStoreRecordData(IsFilterable = true)]`.
### **Creando un endpoint para almacenar `BlogPost` en Qdrant**

Para guardar nuevos `BlogPost`, crearemos un endpoint que recibir치 los datos del post, generar치 su embedding y lo almacenar치 en la base de datos vectorial.

Nuestro servicio ya tiene configurados `ITextEmbeddingGenerationService` (para generar embeddings) e `IVectorStore` (para interactuar con Qdrant).

```csharp
public static class CreateBlogPost
{
    public record Request(string Title, string Description, string[] Tags);

    public record Response(Guid BlogPostId);

    public class Handler
    {
        private readonly ITextEmbeddingGenerationService _embeddingService;
        private readonly IVectorStore _vectorStore;

        public Handler(ITextEmbeddingGenerationService embeddingService, IVectorStore vectorStore)
        {
            _embeddingService = embeddingService;
            _vectorStore = vectorStore;
        }

        public async Task<Response> Handle(Request request, CancellationToken ct)
        {
            var blogposts = _vectorStore.GetCollection<Guid, BlogPost>(BlogPost.VectorName);

            await blogposts.CreateCollectionIfNotExistsAsync(ct);

            var embeddingContents =
                await _embeddingService.GenerateEmbeddingAsync(request.Description, cancellationToken: ct);

            var newBlogPost = new BlogPost
            {
                BlogPostId = Guid.NewGuid(),
                Title = request.Title,
                Description = request.Description,
                DescriptionEmbedding = embeddingContents,
                Tags = request.Tags
            };

            await blogposts.UpsertAsync(newBlogPost, cancellationToken: ct);

            return new Response(newBlogPost.BlogPostId);
        }
    }
}
```


**Explicaci칩n del c칩digo**

1. **Manejador (`Handler`)**
    - Recibe los servicios `ITextEmbeddingGenerationService` y `IVectorStore` a trav칠s del constructor.
2. **L칩gica del m칠todo `Handle`**
    - Obtiene la colecci칩n `blogposts` de Qdrant.
    - Si la colecci칩n no existe, la crea con `CreateCollectionIfNotExistsAsync()`.
    - Genera el embedding del `Description` usando `_embeddingService.GenerateEmbeddingAsync()`.
    - Crea un `BlogPost` con los datos proporcionados y el embedding generado.
    - Guarda el post en Qdrant con `UpsertAsync()`, que inserta o actualiza el registro.

> **Nota 游눠**: Para ver c칩mo este handler se integra con Minimal APIs, revisa el c칩digo fuente del proyecto. Para mantener este art칤culo conciso, omitiremos detalles espec칤ficos.

#### **Probando el endpoint con `api.http`**

Para poblar la base de datos vectorial, podemos hacer una solicitud HTTP de prueba.

Ejemplo en `SemanticKernelLearning03.ApiService.http`:

```
@SemanticKernelLearning03.ApiService_HostAddress = https://localhost:7391

POST {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts
Content-Type: application/json

{
  "Title": "Introducci칩n a ASP.NET Core",
  "Description": "Una gu칤a completa sobre c칩mo comenzar con ASP.NET Core y sus caracter칤sticas principales.",
  "Tags": ["ASP.NET Core", "C#", "Web Development"]
}
```

### **Verificando los datos en Qdrant**

Si ejecutamos nuestra soluci칩n con Aspire y realizamos la solicitud anterior, podemos inspeccionar los datos en el dashboard de Qdrant y verificar que la colecci칩n y los registros existen.

#### **Consideraciones sobre el tama침o de los vectores**

- Este ejemplo usa **nomic-embed-text**, que genera vectores de **768 dimensiones**.
- Qdrant debe estar configurado para aceptar este tama침o de vector.
- Si usas modelos de OpenAI u otros proveedores, verifica el tama침o del vector antes de insertarlo en Qdrant.

## **Recuperando Informaci칩n desde Qdrant**

Para demostrar c칩mo realizar b칰squedas sem치nticas en los vectores almacenados en Qdrant, podemos hacerlo de distintas maneras: usando el vector directamente o utilizando una abstracci칩n que **Semantic Kernel** nos proporciona, llamada `ITextSearch`.

`ITextSearch` tiene distintas implementaciones, como `BingTextSearch` (que realiza b칰squedas en Bing), pero en nuestro caso usaremos `VectorStoreTextSearch`, que est치 dise침ado espec칤ficamente para realizar b칰squedas en bases de datos vectoriales.

Esta clase necesita conocer:

- **El modelo de embeddings** que estamos usando.
- **El nombre de la colecci칩n en Qdrant** donde haremos la b칰squeda.

El proceso realmente es sencillo y podr칤amos hacerlo manualmente construyendo consultas directamente contra la base de datos vectorial. Sin embargo, en este caso, aprovecharemos `ITextSearch` para simplificar el proceso.

> Nota 游눠: Para m치s informaci칩n sobre b칰squedas vectoriales en Semantic Kernel, consulta la documentaci칩n oficial de Microsoft:  
> [Vector search using Semantic Kernel Vector Store connectors (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/vector-search?pivots=programming-language-csharp)

### **Implementaci칩n del Endpoint `SearchBlogPost`**

Para realizar la b칰squeda sem치ntica, crearemos un nuevo endpoint llamado `SearchBlogPost`, que permitir치 encontrar los **posts m치s relevantes** basados en una consulta en lenguaje natural.

```csharp
public static class SearchBlogPost
{
    public record Request(string Query);

    public record Response(Guid BlogPostId, string Title, string Description);

    public class Handler([FromKeyedServices(BlogPost.VectorName)] ITextSearch textSearch)
    {
        public async Task<List<Response>> Handle(Request request, CancellationToken ct)
        {
            KernelSearchResults<TextSearchResult> textResults =
                await textSearch.GetTextSearchResultsAsync(request.Query, new()
                {
                    Top = 2,
                    Skip = 0,
                }, ct);

            List<Response> responses = new();
            await foreach (TextSearchResult result in textResults.Results.WithCancellation(ct))
            {
                responses.Add(new(
                    Guid.Parse(result.Link),
                    result.Name,
                    result.Value));
            }

            return responses;
        }
    }
}
```

**Explicaci칩n del C칩digo**

1. **Uso de `ITextSearch`**
    - Lo recibimos en el constructor con `[FromKeyedServices(BlogPost.VectorName)]`. Esto asegura que estamos inyectando la instancia correcta configurada para nuestra colecci칩n de **blog posts** en Qdrant.
	    - Esto lo hemos configurado en `DependencyConfig` anteriormente.
2. **Realizamos la b칰squeda sem치ntica**
    - Llamamos a `GetTextSearchResultsAsync(...);`, donde:
        - `request.Query` es el t칠rmino de b칰squeda ingresado.
        - `Top = 2` indica que queremos los **dos resultados m치s relevantes**.
        - `Skip = 0` significa que no saltaremos ning칰n resultado.
3. **Procesamos los resultados**
    - Iteramos sobre `textResults.Results` para extraer:
        - `result.Link`: Contiene el `BlogPostId` almacenado como string, lo convertimos a `Guid`.
        - `result.Name`: Corresponde al t칤tulo del blog post.
        - `result.Value`: Contiene la descripci칩n del blog post.
4. **Devolvemos la lista de resultados**
    - Cada `Response` representa un **blog post relevante** basado en la b칰squeda sem치ntica.

Una vez que el endpoint `SearchBlogPost` est치 registrado, podemos comenzar a realizar b칰squedas sem치nticas para encontrar los posts m치s relevantes seg칰n el texto ingresado.

**Ejemplo de Consulta**

Podemos hacer una b칰squeda con la siguiente solicitud:

```
### Busquedas Semanticas
@Query=arquitectura de software
GET {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts?Query={{Query}}
```

**Respuesta Esperada**

Si hay blog posts relevantes en la base de datos, la API devolver치 un listado de los m치s coincidentes. Un ejemplo de respuesta podr칤a ser:

```json
[
먝맡
먝먝먝"blogPostId":"949b7164-2021-4fcc-b58e-9887fdd00d0e",
먝먝먝"title":"Cleanㅁrchitecture:Dise침o맋eSoftwareModular맟Escalable",
먝먝먝"description":"Cleanㅁrchitecture만s맛n만nfoque맋e맋ise침o맋e맙oftware..."
먝맣,
먝맡
먝먝먝"blogPostId":"2340f1ac-e810-4067-90f5-a71899c6d42a",
먝먝먝"title":"PrincipiosSOLID만n만lDesarrollo맋eSoftware",
먝먝먝"description":"Los맗rincipiosSOLID맙on맛n맊onjunto맋e맊inco맗rincipios맋e..."
먝맣
]
```

Si revisamos el repositorio, podemos notar que hay posts de distintos temas (por ejemplo, **Machine Learning, Bases de Datos, DevOps**), pero la b칰squeda regres칩 los m치s relevantes seg칰n el texto ingresado. Esto demuestra que el motor de b칰squeda sem치ntico est치 funcionando correctamente.
#### **Personalizando la B칰squeda**

Podemos ajustar la consulta cambiando:
- **La cantidad de resultados (`Top`)**: Actualmente devuelve **dos posts**, pero podr칤amos aumentar este valor si queremos m치s opciones.
- **El modelo de embeddings**: Si cambiamos el modelo de embeddings usado para almacenar los vectores, los resultados pueden variar en precisi칩n y relevancia.

## **Integraci칩n con RAG (Retrieval-Augmented Generation)**

En esta secci칩n, vamos a implementar un nuevo endpoint que va m치s all치 de una b칰squeda sem치ntica. En lugar de solo buscar contenido relevante, vamos a generar texto de manera din치mica a partir de una consulta proporcionada por el usuario. Lo interesante aqu칤 es que el contexto para la generaci칩n de la respuesta ser치 el contenido de los **Posts del Blog** que coincidan con la consulta del usuario.

### **쮺칩mo funciona?**

Si el usuario hace una pregunta como:

```
cu치l es el objetivo principal de clean architecture?
```

En lugar de simplemente devolver una lista de posts relacionados, el modelo de Semantic Kernel buscar치 los posts m치s relevantes relacionados con la consulta, y usar치 esos resultados como contexto para generar una respuesta detallada. El modelo no solo devolver치 contenido est치tico, sino que tambi칠n incorporar치 la informaci칩n m치s relevante y actualizada disponible en los posts, permitiendo una respuesta m치s precisa y contextualizada.

Esta t칠cnica es 칰til porque, por lo general, los modelos de lenguaje (LLMs) se entrenan con datos est치ticos que no contienen informaci칩n actualizada o espec칤fica del dominio. Al integrar la **Recuperaci칩n de Informaci칩n (RAG)**, aprovechamos los datos disponibles en tiempo real para mejorar la relevancia y exactitud de las respuestas generadas.

### **쮺칩mo lo implementamos?**

En el post anterior, vimos c칩mo crear **Plugins** o **Funciones Sem치nticas**, y c칩mo Semantic Kernel se encarga de invocar autom치ticamente esos plugins cuando es necesario. Ahora, vamos a utilizar un plugin de b칰squeda vectorial para recuperar los posts relevantes y luego construir un **prompt personalizado** que permita al modelo generar una respuesta coherente utilizando esos posts como contexto.

#### **El Endpoint y el C칩digo**

El endpoint que vamos a implementar es el siguiente:

```csharp
public static class QuestionsBlogPost
{
    public record Request(string Query);

    public record Response(string Content);

    public class Handler(Kernel kernel, [FromKeyedServices(BlogPost.VectorName)] ITextSearch textSearch)
    {
        public async Task<Response> Handle(Request request, CancellationToken ct)
        {
            var searchPlugin = textSearch.CreateWithGetTextSearchResults("SearchPlugin");
            kernel.Plugins.Add(searchPlugin);

            string promptTemplate = """
                                    Utiliza solamente los siguientes contenidos para contestar las preguntas realizadas. 
                                    Si no sabes la respuesta, hazle saber al usuario que no se encontr칩 informaci칩n relevante.

                                    Al final de tu respuesta, incluye el nombre del contenido utilizado como referencia.

                                    {{#with (SearchPlugin-GetTextSearchResults query)}}  
                                        {{#each this}}  
                                        Name: {{Name}}
                                        Value: {{Value}}
                                        -----------------
                                        {{/each}}  
                                    {{/with}}  

                                    Pregunta:

                                    {{query}}
                                    """;

            KernelArguments arguments = new() { { "query", request.Query } };

            HandlebarsPromptTemplateFactory promptTemplateFactory = new();
            var response = await kernel.InvokePromptAsync(
                promptTemplate,
                arguments,
                templateFormat: HandlebarsPromptTemplateFactory.HandlebarsTemplateFormat,
                promptTemplateFactory: promptTemplateFactory,
                cancellationToken: ct);

            return new Response(response.ToString());
        }
    }
}

```

**Explicaci칩n del C칩digo**

- **`Handler`**: Esta clase maneja la l칩gica de la consulta, invoca el plugin de b칰squeda, crea el prompt con el contexto adecuado y genera la respuesta. Recibe un objeto **`Kernel`** y una instancia de **`ITextSearch`** que se utiliza para realizar la b칰squeda de contenido relevante en los posts del blog.    
- **`searchPlugin`**: Este es el plugin de b칰squeda que se crea utilizando el m칠todo **`CreateWithGetTextSearchResults`**. Este plugin se encarga de realizar la b칰squeda sem치ntica en los posts y recuperar los resultados m치s relevantes. El plugin se a침ade al **`Kernel`** para que pueda ser invocado cuando sea necesario.
- **`promptTemplate`**: Es el template que se utilizar치 para generar el texto. En este caso, se utiliza **Handlebars** para permitir la inserci칩n de los resultados de la b칰squeda dentro del prompt. La idea es que el modelo genere una respuesta usando solamente los contenidos encontrados en la b칰squeda. El template tambi칠n incluye instrucciones para que el modelo devuelva el nombre y el valor de los posts utilizados como referencia.

```
{{#with (SearchPlugin-GetTextSearchResults query)}}  
    {{#each this}}  
    Name: {{Name}}  
    Value: {{Value}}  
    -----------------
    {{/each}}  
{{/with}}  
```
- Al final, se agrega la pregunta del usuario para que el modelo la utilice como parte de la respuesta:
```
Pregunta:
{{query}}
```
- **`KernelArguments`**: Este objeto contiene los argumentos que se pasan al modelo, en este caso, la consulta realizada por el usuario.
- **`InvokePromptAsync`**: Este m칠todo invoca el modelo utilizando el template y los argumentos proporcionados, lo que genera una respuesta basada en los contenidos recuperados.

**Flujo Completo**

7. **El usuario hace una consulta** (por ejemplo, "쯈u칠 es Clean Architecture?").
8. El **`Kernel`** busca los posts relevantes utilizando el **plugin de b칰squeda vectorial**.
9. El **prompt personalizado** es generado e incluye los resultados de la b칰squeda.
10. El **modelo LLM** genera una respuesta utilizando los posts recuperados como contexto.
11. La **respuesta generada** se devuelve al usuario, incluyendo la referencia a los contenidos utilizados.

Este enfoque permite mejorar la calidad de las respuestas generadas por el modelo, ya que se aprovecha la informaci칩n m치s actualizada y relevante disponible en el dominio. Adem치s, al usar el contenido de los posts, el modelo tiene un contexto m치s espec칤fico y adaptado a las necesidades del usuario, lo que mejora la relevancia y precisi칩n de la respuesta.

**Probando el Endpoint**

Una vez que hemos configurado y creado nuestro endpoint, es hora de probarlo. Para ello, podemos hacer una solicitud HTTP como la siguiente, que ilustrar치 c칩mo interactuar con el servicio de generaci칩n de texto mediante la integraci칩n de RAG.

Ejemplo de solicitud HTTP:

```
###
@Question=cu치l es el objetivo principal de clean architecture?
GET {{SemanticKernelLearning03.ApiService_HostAddress}}/api/blog-posts/qa?Query={{Question}}
```

Cuando se ejecuta esta solicitud, obtendremos una respuesta estructurada que incluye no solo la respuesta generada por el modelo, sino tambi칠n la referencia al contenido utilizado para generar esa respuesta.

Respuesta esperada:

```
El맖bjetivo맗rincipal맋eCleanㅁrchitecture만s맗romover맓a맙eparaci칩n맋e맗reocupaciones맟맓a맔odularidad만n만l맋ise침o맋e맙oftware,맓o맘ue맗ermite마맓as마plicaciones맙er맔치s맔antenibles맟만scalables마맓o맓argo맋el맚iempo.

Reference:Cleanㅁrchitecture:Dise침o맋eSoftwareModular맟Escalable.
```

En este caso, la respuesta del endpoint es completamente din치mica y se genera utilizando los resultados de b칰squeda obtenidos de los posts del blog relacionados con la pregunta. Como puedes observar, no solo obtenemos un resultado que responde a la consulta, sino que tambi칠n proporcionamos el nombre del contenido utilizado como referencia, lo cual permite al usuario conocer la fuente de la informaci칩n generada.

En el ejemplo, configuramos el `promptTemplate` para que el modelo de lenguaje solo utilizara los contenidos relevantes encontrados a trav칠s de la b칰squeda de texto vectorial. De esta forma, garantizamos que la generaci칩n del modelo sea espec칤fica y relevante para el contexto proporcionado por los posts del blog.

Este template establece claramente la regla para que el modelo solo use los resultados de b칰squeda como contexto, lo cual optimiza la relevancia de las respuestas generadas.

**Escalabilidad y Flexibilidad del Prompt**

El prompt puede ser adaptado para ser m치s espec칤fico y mejorar la calidad de las respuestas generadas, por ejemplo, agregando instrucciones m치s detalladas sobre el formato de la respuesta o proporcionando una estructura m치s precisa para guiar al modelo. La flexibilidad del sistema permite ajustar el nivel de detalle seg칰n la necesidad del caso de uso.

**Ejemplo de otro Prompt:**

```
Enlista los puntos m치s importantes al querer optimizar consultas SQL
```

**Resultado generado:**

```
춰Claro! A continuaci칩n, te presento los puntos m치s importantes para optimizar consultas SQL:

1. **칈ndices**: Los 칤ndices ayudan a acelerar la b칰squeda de datos en una base de datos. Deben ser creados en las columnas que se utilizan en la cl치usula WHERE o en la cl치usula ORDER BY.
2. **Normalizaci칩n y desnormalizaci칩n de datos**: La normalizaci칩n ayuda a reducir la cantidad de datos duplicados en una base de datos, mientras que la desnormalizaci칩n puede mejorar el rendimiento en ciertas situaciones.
.
.
.

Referencia: Optimizaci칩n de Consultas SQL en Aplicaciones .NET
```

Como vemos, el modelo genera una lista de los puntos m치s importantes y proporciona la referencia adecuada para la informaci칩n. Este tipo de integraci칩n con Semantic Kernel y RAG hace que sea posible tener respuestas altamente relevantes y espec칤ficas basadas en contenido actual y relacionado.
## Conclusi칩n

A lo largo de este post, hemos aprendido a configurar y utilizar Semantic Kernel para aprovechar el poder de los embeddings y la t칠cnica de Retrieval-Augmented Generation (RAG). Desde la generaci칩n y almacenamiento de vectores en Qdrant, hasta la realizaci칩n de b칰squedas sem치nticas y la generaci칩n de respuestas contextualizadas mediante prompts personalizados, hemos visto c칩mo combinar distintas tecnolog칤as para crear soluciones de IA avanzadas y especializadas.

Esta integraci칩n no solo mejora la precisi칩n y relevancia de las respuestas, sino que tambi칠n demuestra el potencial de unir funciones sem치nticas, RAG y bases de datos vectoriales para abordar desaf칤os reales en el manejo de informaci칩n. Con estas herramientas, podr치s desarrollar aplicaciones que se adaptan al contexto de tus datos, ofreciendo respuestas din치micas y actualizadas.

춰El camino hacia aplicaciones inteligentes y contextualmente precisas est치 abierto! Sigue experimentando y optimizando estas t칠cnicas para llevar tus proyectos al siguiente nivel.

## Referencias

- [Using the Semantic Kernel Vector Store text search (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/out-of-the-box-textsearch/vectorstore-textsearch?pivots=programming-language-csharp)
- [Semantic Kernel Text Search Plugins (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/text-search-plugins?pivots=programming-language-csharp)
- [Semantic Kernel Text Search with Vector Stores (Preview) | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/text-search/text-search-vector-stores?pivots=programming-language-csharp)
- [Using the Handlebars prompt template language | Microsoft Learn](https://learn.microsoft.com/en-us/semantic-kernel/concepts/prompts/handlebars-prompt-templates?pivots=programming-language-csharp)