### **1. IntroducciÃ³n**

En este tutorial, aprenderÃ¡s cÃ³mo usar **Semantic Kernel** para integrar Large Language Models (LLM) y construir un servicio REST que resuma texto. Utilizaremos **Ollama** como motor local de modelos de lenguaje, lo que nos permitirÃ¡ evitar el uso de servicios en la nube en esta fase de desarrollo.

El objetivo principal es configurar un entorno de trabajo funcional en C# que permita a los desarrolladores:

1. Conectar un modelo de lenguaje local (_llama3.2_).
2. Crear un kernel para manejar tareas personalizadas.
3. Exponer la funcionalidad como un endpoint REST en una API ASP.NET Core.

Al finalizar, tendrÃ¡s un servicio de API que podrÃ¡ recibir texto como entrada y devolver un resumen en una sola oraciÃ³n. Esto no solo te ayudarÃ¡ a entender cÃ³mo usar Semantic Kernel, sino que tambiÃ©n te darÃ¡ una base sÃ³lida para construir aplicaciones prÃ¡cticas basadas en IA generativa.

### **2. IntroducciÃ³n a Semantic Kernel**

#### **2.1. Â¿QuÃ© es Semantic Kernel?**

Semantic Kernel es un SDK de cÃ³digo abierto que permite a los desarrolladores crear sus propios agentes personalizados de inteligencia artificial (IA). Al combinar modelos de lenguaje de gran escala (LLMs) con cÃ³digo nativo, los desarrolladores pueden crear agentes de IA que entiendan y respondan a solicitudes en lenguaje natural para realizar una variedad de tareas.

#### **2.2 Â¿QuÃ© es un agente de IA?**

Un agente de IA es un programa diseÃ±ado para alcanzar objetivos predeterminados. Los agentes de IA estÃ¡n impulsados por modelos de lenguaje de gran escala (LLMs) entrenados con cantidades masivas de datos. Estos agentes pueden completar una amplia variedad de tareas con mÃ­nima o ninguna intervenciÃ³n humana. Algunos ejemplos de lo que pueden hacer los agentes de IA incluyen:

- Escribir cÃ³digo.
- Redactar correos electrÃ³nicos.
- Resumir reuniones.
- Proporcionar recomendaciones.
- Â¡Y mucho mÃ¡s!

Semantic Kernel integra modelos de lenguaje como OpenAI, Azure OpenAI y Hugging Face con lenguajes de programaciÃ³n convencionales como C#, Python y Java. Los desarrolladores pueden crear "plugins" para interactuar con los LLMs y realizar diversas tareas. AdemÃ¡s, el SDK de Semantic Kernel incluye plugins predefinidos que pueden mejorar rÃ¡pidamente una aplicaciÃ³n. Esto permite que los desarrolladores utilicen LLMs en sus propias aplicaciones sin necesidad de aprender los detalles especÃ­ficos de la API del modelo.

#### **2.3 Componentes clave del SDK de Semantic Kernel**

**Capa de orquestaciÃ³n de IA**  
El nÃºcleo del stack de Semantic Kernel es una capa de orquestaciÃ³n de IA que permite la integraciÃ³n fluida de modelos de IA y plugins. Esta capa combina estos componentes para crear interacciones innovadoras con los usuarios.

**Conectores**  
El SDK ofrece un conjunto de conectores que permiten a los desarrolladores integrar LLMs en sus aplicaciones existentes. Estos conectores actÃºan como un puente entre el cÃ³digo de la aplicaciÃ³n y los modelos de IA.

**Plugins**  
El SDK opera con plugins, que funcionan como el "cuerpo" de la aplicaciÃ³n de IA. Los plugins incluyen solicitudes (prompts) que el modelo de IA debe responder y funciones para realizar tareas especializadas. Los desarrolladores pueden usar plugins predefinidos o crear los suyos propios.
### **3. Desarrollo del Ejemplo**

En esta secciÃ³n, construiremos un API REST que utiliza **Semantic Kernel** y el modelo de lenguaje **llama3.2** de **Ollama** para resumir texto en una sola oraciÃ³n. El objetivo es entender cÃ³mo configurar el entorno y crear una habilidad bÃ¡sica que pueda ser utilizada desde cualquier aplicaciÃ³n a travÃ©s de un endpoint.

#### **3.1. ConfiguraciÃ³n con Aspire**

Antes de entrar en los detalles del API, configuramos el entorno utilizando **Aspire**. Este framework facilita la construcciÃ³n de aplicaciones distribuidas, simplificando la gestiÃ³n de dependencias y servicios.

Lo que he hecho para simplificar la configuraciÃ³n, es utilizar Visual Studio para crear la soluciÃ³n ejemplo de Aspire con .NET 9:

![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/9av06ysmd1bwoiyyj7is.png)

Esta plantilla, tambiÃ©n incluye un proyecto de Blazor, en este tutorial no lo utilizaremos, pero al finalizar, puedes utilizar ese proyecto para consumir los endpoints realizados y ya tener una aplicaciÃ³n funcional.

El cÃ³digo de Aspire tiene los siguientes propÃ³sitos:

- **Configurar Ollama:** Define el modelo `llama3.2` como el motor de lenguaje. Esto nos permite trabajar con un modelo local, eliminando la necesidad de infraestructura compleja en desarrollo.
- **Modularidad:** Cada proyecto (como `SemanticKernelLearning01_ApiService`) se configura como un mÃ³dulo independiente, permitiendo una estructura clara y extensible.

**Nota:** Usar Ollama local es ideal para pruebas y desarrollo, ya que no requiere configurar una infraestructura en la nube. Sin embargo, en producciÃ³n puedes cambiar a OpenAI o Azure OpenAI sin modificar el cÃ³digo, aprovechando la flexibilidad de Semantic Kernel.

#### **CÃ³digo de Aspire:**

Primero necesitamos los siguientes paquetes:

```xml
  <ItemGroup>
    <PackageReference Include="Aspire.Hosting.AppHost" Version="9.0.0" />
    <PackageReference Include="CommunityToolkit.Aspire.Hosting.Ollama" Version="9.1.0" />
  </ItemGroup>
```

```csharp
var builder = DistributedApplication.CreateBuilder(args);

var ollama =
    builder
        .AddOllama("ollama")// <-- UtilizarÃ¡ docker para usar ollama y sus modelos 
        .WithDataVolume()   // <-- Volumen de Docker para persistir modelos descargados
        .WithOpenWebUI();   // <-- UI Estilo ChatGPT

// Descarga el modelo llama3.2 con nombre "llama"
var llamaModel = ollama.AddModel("llama", "llama3.2");

// Nuestra API depende de Ollama, por lo que se referencia y espera a que estÃ© listo
builder.AddProject<Projects.SemanticKernelLearning01_ApiService>("apiservice")
    .WithReference(llamaModel)
    .WaitFor(llamaModel);

builder.Build().Run();
```

AquÃ­ configuramos Ollama como motor de generaciÃ³n de texto y vinculamos el modelo `llama3.2`. TambiÃ©n iniciamos el servicio API como un mÃ³dulo dentro del proyecto.

#### **3.2. ConfiguraciÃ³n del API REST**

En el siguiente bloque de cÃ³digo, configuramos un API REST utilizando **ASP.NET Core** y **Semantic Kernel**. AquÃ­ se define un endpoint que acepta texto como entrada y devuelve su resumen generado por el modelo de lenguaje.

**Puntos clave del cÃ³digo:**

1. **Kernel y Ollama:** Se inicializa el kernel de Semantic Kernel y se conecta con Ollama usando la configuraciÃ³n definida en Aspire.
2. **Habilidad de Resumen:** La lÃ³gica para resumir texto se encapsula en un prompt que el modelo interpreta para generar respuestas.
3. **ExposiciÃ³n del Endpoint:** Se define un endpoint `/api/summarizer` que recibe un objeto JSON con el texto a resumir y devuelve el resultado.

#### **CÃ³digo del API REST:**

Paquetes necesarios:

```xml
  <ItemGroup>
    <PackageReference Include="Microsoft.AspNetCore.OpenApi" Version="9.0.0" />
    <PackageReference Include="Microsoft.SemanticKernel" Version="1.33.0" />
    <PackageReference Include="Microsoft.SemanticKernel.Connectors.Ollama" Version="1.33.0-alpha" />
  </ItemGroup>
```

**Program.cs**

```csharp
var builder = WebApplication.CreateBuilder(args);

builder.AddServiceDefaults();
builder.Services.AddProblemDetails();
builder.Services.AddOpenApi();

var kernel = builder.Services.AddKernel();

var (endpoint, modelId) = GetOllamaConnectionString();

#pragma warning disable SKEXP0070
kernel.AddOllamaTextGeneration(modelId, endpoint); // EstÃ¡ en alpha, puede cambiar.
#pragma warning restore SKEXP0070

var app = builder.Build();

app.UseExceptionHandler();

if (app.Environment.IsDevelopment())
{
    app.MapOpenApi();
}

app.MapDefaultEndpoints();

app.MapPost("/api/summarizer", async (
    TextCompletionRequest request,
    ITextGenerationService textGenerationService) =>
{
    var prompt = $"""
                 Summarize the following text in one sentence:  

                 {request.Text}
                 """;

    var response = await textGenerationService.GetTextContentsAsync(prompt);

    return response;
});

app.Run();

```

#### **3.3. ExplicaciÃ³n General del Flujo**

1. **CreaciÃ³n del Kernel:**  
    Se configura el kernel para usar Ollama como motor de generaciÃ³n de texto con el modelo `llama3.2`.
2. **DefiniciÃ³n del Endpoint:**  
    El endpoint `/api/summarizer` recibe un JSON con una propiedad `Text`.
3. **GeneraciÃ³n del Resumen:**  
    El texto recibido se envÃ­a al modelo junto con un prompt que instruye al modelo para resumirlo en una oraciÃ³n.
4. **DevoluciÃ³n del Resultado:**  
    El resumen generado se devuelve como respuesta al cliente que llamÃ³ al API.

CÃ³digo restante:

```csharp
(Uri endpoint, string modelId) GetOllamaConnectionString()
{
    // Este ConnectionString es establecida por Aspire
    var connectionString = builder.Configuration.GetConnectionString("llama");

    var connectionBuilder = new DbConnectionStringBuilder
    {
        ConnectionString = connectionString
    };

    Uri endpoint = new Uri(connectionBuilder["Endpoint"].ToString());
    string modelId = connectionBuilder["Model"].ToString();

    return (endpoint, modelId);
}

// Simple DTO
public record TextCompletionRequest(string Text);
```

> Nota ğŸ’¡: Recuerda que siempre puedes descargar el cÃ³digo desde desde este [Repositorio](https://github.com/isaacOjeda/DevToPosts/tree/main/SemanticKernelSeries/SemanticKernelLearning01)
> 

#### **3.4 Probando la soluciÃ³n** 

Si corremos el proyecto de Aspire, automÃ¡ticamente orquestarÃ¡ lo necesario para que la API pueda comunicarse con ollama:


![Image description](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/tv7ykzjwjcw13l0q4l7h.png)

Su primera ejecuciÃ³n serÃ¡ lenta, ya que tendrÃ¡ que descargar el modelo `llama3.2`.

Existen muchos modelos que podemos ejecutar, tanto para Text Generation y para Embeddings (revisado en prÃ³xmos post), puedes revisar mÃ¡s aquÃ­: [Ollama](https://ollama.com/).

Probemos el endpoint (utilizando Visual Studio y un archivo .http):

```
@host = https://localhost:7384

### Text Generation
POST {{host}}/api/summarizer
Content-Type: application/json

{
    "text": "Mariana found an ancient book in her grandfatherâ€™s library, with a bookmark pointing to a page that read, â€œRecite these words and your destiny will change.â€ Intrigued, she read it out loud, and in an instant, she found herself in a bustling market in a medieval town. \rNo one seemed surprised by her modern clothing; in fact, one merchant greeted her as if he knew her. As she searched for answers, an old man explained that she was the heir to a lost legacy and had to make a choice: stay and lead a kingdom on the brink of chaos, or return to her everyday life. \rWith the book in her hands, Mariana took a deep breath and closed her eyes, choosing the challenge she had secretly always wanted."
}

```

Respuesta:

```json
[
Â Â {
Â Â Â Â "text":Â "MarianaÂ discoversÂ anÂ ancientÂ bookÂ thatÂ transportsÂ herÂ toÂ aÂ medievalÂ town,Â whereÂ sheÂ learnsÂ sheÂ isÂ theÂ heirÂ toÂ aÂ lostÂ legacyÂ andÂ mustÂ makeÂ aÂ choiceÂ betweenÂ returningÂ toÂ herÂ ordinaryÂ lifeÂ orÂ leadingÂ aÂ kingdomÂ onÂ theÂ brinkÂ ofÂ chaos.",
Â Â Â Â "modelId":Â "llama3.2"
Â Â }
]
```

La ejecuciÃ³n de modelos con ollama claro que suele ser mÃ¡s lento, para acelerar su ejecuciÃ³n debemos de contar con una GPU y de preferencia Nvidia. 

Para operaciones sencillas, me ha funcionado bien y como vemos, la respuesta ha funcionado, todo ejecutandose en nuestra computadora.
### **4. ConclusiÃ³n**

En este tutorial, exploramos los fundamentos de **Semantic Kernel** y cÃ³mo integrarlo con **Ollama** para construir un servicio funcional capaz de generar resÃºmenes de texto. A travÃ©s de este ejercicio:

- Aprendiste a configurar un entorno bÃ¡sico utilizando **Aspire**, simplificando la administraciÃ³n de servicios distribuidos.
- Descubriste cÃ³mo conectar Semantic Kernel con un motor de lenguaje local, como **llama3.2** de Ollama, para evitar complicaciones de infraestructura en desarrollo.
- Implementaste un API REST con un endpoint prÃ¡ctico que utiliza generaciÃ³n de texto como una habilidad central.

Este ejercicio muestra lo versÃ¡til que puede ser Semantic Kernel para crear aplicaciones inteligentes que aprovechan los modelos de lenguaje. AdemÃ¡s, la flexibilidad de la arquitectura permite cambiar fÃ¡cilmente entre diferentes proveedores de modelos, como OpenAI o Azure, sin necesidad de modificar el cÃ³digo base.

Este tutorial es solo el inicio. A partir de aquÃ­, puedes explorar y agregar nuevas habilidades, integrar bases de datos vectoriales para contextualizar las respuestas o expandir el API con capacidades adicionales.

### **5. PrÃ³ximos Pasos**

Ahora que ya tienes una base sÃ³lida para trabajar con **Semantic Kernel** y **Ollama**, considera avanzar con los siguientes temas para expandir tus conocimientos:

1. **Ampliar tus habilidades en Semantic Kernel:**
    - Aprende a crear habilidades mÃ¡s complejas con mÃºltiples pasos y dependencias.
    - Explora cÃ³mo usar conectores para integrar datos externos, como archivos, APIs, o bases de datos.
2. **Incorporar contexto mediante bases de datos vectoriales:**
    - Experimenta con herramientas como Qdrant o Pinecone para agregar contexto a las respuestas basadas en vectores.
    - Usa embeddings para conectar preguntas con datos relevantes en tiempo real.
3. **Migrar a un entorno de producciÃ³n:**
    - Configura la misma API para usar OpenAI o Azure OpenAI como motores de generaciÃ³n de texto.
    - Aprende a manejar credenciales y seguridad en aplicaciones en producciÃ³n.
4. **Construir una interfaz grÃ¡fica:**
    - Usa **Blazor** para crear una interfaz interactiva donde los usuarios puedan interactuar con tus habilidades de Semantic Kernel.
5. **Desarrollar tutoriales mÃ¡s avanzados:**
    - EnseÃ±a a otros cÃ³mo resolver problemas reales, como anÃ¡lisis de sentimientos, clasificaciÃ³n de texto o generaciÃ³n de contenido adaptado al contexto.
